<!DOCTYPE html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#2D2D2D" />
  
  <title>SpanMarker :: Initializing &amp; Training with SpanMarker</title>
  
  <link rel="index" title="Index" href="../genindex.html"/>

  <link rel="stylesheet" href="../_static/css/nltk_theme.css"/>
  <link rel="stylesheet" href="../_static/css/custom.css"/>

  <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="../_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
  

  <script src="https://email.tl.fortawesome.com/c/eJxNjUEOgyAQAF8jR7Kw6wIHDh7sP1Cw2mgxgmn6-3JsMqc5zEQfE8dkxOY1KKMUOI3ACFKRJpSW2AAp7ontYIaxI6i7XPJVwyeVfCQ550Os3jLrGSNOLgbdAy6s0PBk2TFNjEbsfq31LB0OnX407pJa5v2faRadwSW63mn5KuLyR9j2tgx3zecanl-55R_-jjPs"></script> 
</head>

<body>
  <div id="nltk-theme-container">
    <header>
      <div id="logo-container">
          
          <h1>
            <a href="../index.html">SpanMarker</a>
          </h1>
          
      </div>
      <div id="project-container">
        
        <h1>Documentation</h1>
        
      </div>

      <a id="menu-toggle" class="fa fa-bars" aria-hidden="true"></a>

      <script type="text/javascript">
        $("#menu-toggle").click(function() {
          $("#menu-toggle").toggleClass("toggled");
          $("#side-menu-container").slideToggle(300);
        });
      </script>
    </header>

    <div id="content-container">

      <div id="side-menu-container">

        <div id="search" role="search">
        <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
            <input type="text" name="q" placeholder="Search" />
            <input type="hidden" name="check_keywords" value="yes" />
            <input type="hidden" name="area" value="default" />
        </form>
</div>

        <div id="side-menu" role="navigation">
          
  
    
  
  
    <p class="caption" role="heading"><span class="caption-text">Documentation</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Notebooks</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="getting_started.html">Getting Started</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Initializing &amp; Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_loading.html">Loading &amp; Inferencing</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html">Configuring</a></li>
<li class="toctree-l2"><a class="reference internal" href="spacy_integration.html">SpanMarker with spaCy</a></li>
<li class="toctree-l2"><a class="reference internal" href="document_level_context.html">Document-level context</a></li>
<li class="toctree-l2"><a class="reference external" href="https://raw.githubusercontent.com/tomaarsen/SpanMarkerNER/main/thesis.pdf">SpanMarker Thesis</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api/span_marker.html">API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/span_marker.modeling.html">span_marker.modeling module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/span_marker.trainer.html">span_marker.trainer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/span_marker.configuration.html">span_marker.configuration module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/span_marker.model_card.html">span_marker.model_card module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/span_marker.pipeline_component.html">span_marker.pipeline_component module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/span_marker.data_collator.html">span_marker.data_collator module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/span_marker.tokenizer.html">span_marker.tokenizer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/span_marker.evaluation.html">span_marker.evaluation module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/span_marker.label_normalizer.html">span_marker.label_normalizer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/span_marker.output.html">span_marker.output module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/span_marker.spacy_integration.html">span_marker.spacy_integration module</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing SpanMarker</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../news.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/tomaarsen/SpanMarkerNER/issues">Open Issues</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/tomaarsen/SpanMarkerNER">SpanMarker on GitHub</a></li>
<li class="toctree-l1"><a class="reference external" href="https://huggingface.co/models?library=span-marker">SpanMarker on the 🤗 Hub</a></li>
<li class="toctree-l1"><a class="reference external" href="https://spacy.io/universe/project/span_marker">SpanMarker on spaCy</a></li>
</ul>

  

        </div>

        
      </div>

      <div id="main-content-container">
        <div id="main-content" role="main">
          
  <div class="open-in-colab__wrapper">
<a href="https://colab.research.google.com/github/tomaarsen/SpanMarkerNER/blob/main/notebooks/model_training.ipynb" target="_blank">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" style="display: inline; margin: 0" alt="Open In Colab"/>
</a>
</div><section id="Initializing-&amp;-Training-with-SpanMarker">
<h1>Initializing &amp; Training with SpanMarker<a class="headerlink" href="#Initializing-&-Training-with-SpanMarker" title="Permalink to this heading">¶</a></h1>
<p><a class="reference external" href="https://github.com/tomaarsen/SpanMarkerNER">SpanMarker</a> is an accessible yet powerful Python module for training Named Entity Recognition models.</p>
<p>In this short notebook, we’ll have a look at how to initialize and train an NER model using SpanMarker. For a larger and more general tutorial on how to use SpanMarker, please have a look at the <a class="reference internal" href="getting_started.html"><span class="doc">Getting Started</span></a> notebook.</p>
<section id="Setup">
<h2>Setup<a class="headerlink" href="#Setup" title="Permalink to this heading">¶</a></h2>
<p>First of all, the <code class="docutils literal notranslate"><span class="pre">span_marker</span></code> Python module needs to be installed. If we want to use <a class="reference external" href="https://wandb.ai/">Weights and Biases</a> for logging, we can install <code class="docutils literal notranslate"><span class="pre">span_marker</span></code> using the <code class="docutils literal notranslate"><span class="pre">[wandb]</span></code> extra.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">pip</span> install span_marker
<span class="c1"># %pip install span_marker[wandb]</span>
</pre></div>
</div>
</div>
</section>
<section id="Loading-the-dataset">
<h2>Loading the dataset<a class="headerlink" href="#Loading-the-dataset" title="Permalink to this heading">¶</a></h2>
<p>For this example, we’ll load the commonly used <a class="reference external" href="https://huggingface.co/datasets/conll2003">CoNLL2003 dataset</a> from the Hugging Face hub using 🤗 Datasets.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">dataset_id</span> <span class="o">=</span> <span class="s2">&quot;conll2003&quot;</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="n">dataset_id</span><span class="p">)</span>
<span class="n">dataset</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
DatasetDict({
    train: Dataset({
        features: [&#39;id&#39;, &#39;tokens&#39;, &#39;pos_tags&#39;, &#39;chunk_tags&#39;, &#39;ner_tags&#39;],
        num_rows: 14041
    })
    validation: Dataset({
        features: [&#39;id&#39;, &#39;tokens&#39;, &#39;pos_tags&#39;, &#39;chunk_tags&#39;, &#39;ner_tags&#39;],
        num_rows: 3250
    })
    test: Dataset({
        features: [&#39;id&#39;, &#39;tokens&#39;, &#39;pos_tags&#39;, &#39;chunk_tags&#39;, &#39;ner_tags&#39;],
        num_rows: 3453
    })
})
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">labels</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="s2">&quot;ner_tags&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">feature</span><span class="o">.</span><span class="n">names</span>
<span class="n">labels</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[&#39;O&#39;, &#39;B-PER&#39;, &#39;I-PER&#39;, &#39;B-ORG&#39;, &#39;I-ORG&#39;, &#39;B-LOC&#39;, &#39;I-LOC&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;]
</pre></div></div>
</div>
<p>SpanMarker accepts any dataset as long as it has <code class="docutils literal notranslate"><span class="pre">tokens</span></code> and <code class="docutils literal notranslate"><span class="pre">ner_tags</span></code> columns. The <code class="docutils literal notranslate"><span class="pre">ner_tags</span></code> can be annotated using the IOB, IOB2, BIOES or BILOU labeling scheme, but also regular unschemed labels. This CoNLL dataset uses the common IOB or IOB2 labeling scheme, with PER, ORG, LOC and MISC labels.</p>
</section>
<section id="Initializing-a-SpanMarkerModel">
<h2>Initializing a <code class="docutils literal notranslate"><span class="pre">SpanMarkerModel</span></code><a class="headerlink" href="#Initializing-a-SpanMarkerModel" title="Permalink to this heading">¶</a></h2>
<p>A SpanMarker model is initialized via <a class="reference external" href="https://tomaarsen.github.io/SpanMarkerNER/api/span_marker.modeling.html#span_marker.modeling.SpanMarkerModel.from_pretrained">SpanMarkerModel.from_pretrained</a>. This method will be familiar to those who know 🤗 Transformers. It accepts either a path to a local model or the name of a model on the <a class="reference external" href="https://huggingface.co/models">Hugging Face Hub</a>.</p>
<p>Importantly, the model can <em>either</em> be an encoder or an already trained and saved SpanMarker model. As we haven’t trained anything yet, we will use an encoder. To learn how to load and use a saved SpanMarker model, please have a look at the <a class="reference internal" href="model_loading.html"><span class="doc">Loading &amp; Inferencing</span></a> notebook.</p>
<p>Reasonable options for encoders include BERT and RoBERTa, which means that the following are all good options:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/prajjwal1/bert-tiny">prajjwal1/bert-tiny</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/prajjwal1/bert-mini">prajjwal1/bert-mini</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/prajjwal1/bert-small">prajjwal1/bert-small</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/prajjwal1/bert-medium">prajjwal1/bert-medium</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/bert-base-cased">bert-base-cased</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/bert-large-cased">bert-large-cased</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/bert-base-multilingual-cased">bert-base-multilingual-cased</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/bert-base-multilingual-uncased">bert-base-multilingual-uncased</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/roberta-base">roberta-base</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/roberta-large">roberta-large</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/xlm-roberta-base">xlm-roberta-base</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/xlm-roberta-large">xlm-roberta-large</a></p></li>
</ul>
<p>Not all encoders work though, they <strong>must</strong> allow for <code class="docutils literal notranslate"><span class="pre">position_ids</span></code> as an input argument, which disqualifies DistilBERT, T5, DistilRoBERTa, ALBERT &amp; BART.</p>
<p>Additionally, it’s important to consider that cased models typically demand consistent capitalization in the inference data, aligning with how the training data is formatted. In simpler terms, if your training data consistently uses correct capitalization, but your inference data does not, it may lead to suboptimal performance. In such cases, you might find an uncased model more suitable. Although it may exhibit slightly lower F1 scores on the testing set, it remains functional regardless of
capitalization, making it potentially more effective in real-world scenarios.</p>
<p>We’ll use <code class="docutils literal notranslate"><span class="pre">&quot;roberta-base&quot;</span></code> for this notebook. If you’re running this on Google Colab, be sure to set hardware accelerator to “GPU” in <code class="docutils literal notranslate"><span class="pre">Runtime</span></code> &gt; <code class="docutils literal notranslate"><span class="pre">Change</span> <span class="pre">runtime</span> <span class="pre">type</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">span_marker</span><span class="w"> </span><span class="kn">import</span> <span class="n">SpanMarkerModel</span><span class="p">,</span> <span class="n">SpanMarkerModelCardData</span>

<span class="n">encoder_id</span> <span class="o">=</span> <span class="s2">&quot;roberta-base&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SpanMarkerModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="c1"># Required arguments</span>
    <span class="n">encoder_id</span><span class="p">,</span>
    <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
    <span class="c1"># Optional arguments</span>
    <span class="n">model_max_length</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">entity_max_length</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="c1"># To improve the generated model card</span>
    <span class="n">model_card_data</span><span class="o">=</span><span class="n">SpanMarkerModelCardData</span><span class="p">(</span>
        <span class="n">language</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;en&quot;</span><span class="p">],</span>
        <span class="n">license</span><span class="o">=</span><span class="s2">&quot;apache-2.0&quot;</span><span class="p">,</span>
        <span class="n">encoder_id</span><span class="o">=</span><span class="n">encoder_id</span><span class="p">,</span>
        <span class="n">dataset_id</span><span class="o">=</span><span class="n">dataset_id</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: [&#39;roberta.pooler.dense.bias&#39;, &#39;roberta.pooler.dense.weight&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre></div></div>
</div>
<p>For us, these warnings are expected, as we are initializing <code class="docutils literal notranslate"><span class="pre">RobertaModel</span></code> for a new task.</p>
<p>Note that we provided <code class="docutils literal notranslate"><span class="pre">SpanMarkerModel.from_pretrained</span></code> with a list of our labels. This is required when training a new model using an encoder. Furthermore, we can specify some useful configuration parameters from <code class="docutils literal notranslate"><span class="pre">SpanMarkerConfig</span></code>, such as:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model_max_length</span></code>: The maximum number of tokens that the model will process. If you only use short sentences for your model, reducing this number may help training and inference speeds with no loss in performance. Defaults to the encoder maximum, or 512 if the encoder doesn’t have a maximum.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">entity_max_length</span></code>: The total number of words that one entity can be. Defaults to 8.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_card_data</span></code>: A <a class="reference external" href="https://tomaarsen.github.io/SpanMarkerNER/api/span_marker.model_card.html#span_marker.model_card.SpanMarkerModelCardData">SpanMarkerModelCardData</a> instance where you can provide a lot of useful data about your model. This data will be automatically included in a generated model card whenever a model is saved or pushed to the Hugging Face Hub.</p>
<ul>
<li><p>Consider adding <code class="docutils literal notranslate"><span class="pre">language</span></code>, <code class="docutils literal notranslate"><span class="pre">license</span></code>, <code class="docutils literal notranslate"><span class="pre">model_id</span></code>, <code class="docutils literal notranslate"><span class="pre">encoder_id</span></code> and <code class="docutils literal notranslate"><span class="pre">dataset_id</span></code> to improve the generated model card README.md file.</p></li>
</ul>
</li>
</ul>
</section>
<section id="Training">
<h2>Training<a class="headerlink" href="#Training" title="Permalink to this heading">¶</a></h2>
<p>At this point, our model is already ready for training! We can import <a class="reference external" href="https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments">TrainingArguments</a> directly from 🤗 Transformers as well as the SpanMarker <code class="docutils literal notranslate"><span class="pre">Trainer</span></code>. The <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> is a subclass of the 🤗 Transformers <a class="reference external" href="https://huggingface.co/docs/transformers/main_classes/trainer">Trainer</a> that simplifies some tasks for you, but otherwise it works just like the regular <code class="docutils literal notranslate"><span class="pre">Trainer</span></code>.</p>
<p>This next snippet shows some reasonable defaults. Feel free to adjust the batch size to a lower value if you experience out of memory exceptions.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">TrainingArguments</span>

<span class="n">args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;models/span-marker-roberta-base-conll03&quot;</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s2">&quot;steps&quot;</span><span class="p">,</span>
    <span class="n">save_strategy</span><span class="o">=</span><span class="s2">&quot;steps&quot;</span><span class="p">,</span>
    <span class="n">eval_steps</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">warmup_ratio</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now we can create a SpanMarker <a class="reference external" href="https://tomaarsen.github.io/SpanMarkerNER/api/span_marker.trainer.html#span_marker.trainer.Trainer">Trainer</a> in the same way that you would initialize a 🤗 Transformers <code class="docutils literal notranslate"><span class="pre">Trainer</span></code>. We’ll train on a subsection of the data to save us some time. Amazingly, this <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> will automatically create logs using exactly the logging tools that you have installed. With other words, if you prefer logging with <a class="reference external" href="https://www.tensorflow.org/tensorboard">Tensorboard</a>,
all that you have to do is install it.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">span_marker</span><span class="w"> </span><span class="kn">import</span> <span class="n">Trainer</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">],</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;validation&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">)),</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
This SpanMarker model will ignore 0.097877% of all annotated entities in the train dataset. This is caused by the SpanMarkerModel maximum entity length of 6 words.
These are the frequencies of the missed entities due to maximum entity length out of 23499 total entities:
- 18 missed entities with 7 words (0.076599%)
- 2 missed entities with 8 words (0.008511%)
- 3 missed entities with 10 words (0.012767%)
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;loss&#39;: 1.1135, &#39;learning_rate&#39;: 2.707182320441989e-06, &#39;epoch&#39;: 0.03}
{&#39;loss&#39;: 0.245, &#39;learning_rate&#39;: 5.469613259668509e-06, &#39;epoch&#39;: 0.06}
{&#39;loss&#39;: 0.1466, &#39;learning_rate&#39;: 8.232044198895029e-06, &#39;epoch&#39;: 0.08}
{&#39;loss&#39;: 0.1077, &#39;learning_rate&#39;: 9.888957433682912e-06, &#39;epoch&#39;: 0.11}
{&#39;loss&#39;: 0.0839, &#39;learning_rate&#39;: 9.58050586057989e-06, &#39;epoch&#39;: 0.14}
{&#39;loss&#39;: 0.0702, &#39;learning_rate&#39;: 9.272054287476866e-06, &#39;epoch&#39;: 0.17}
{&#39;loss&#39;: 0.0614, &#39;learning_rate&#39;: 8.963602714373844e-06, &#39;epoch&#39;: 0.19}
{&#39;loss&#39;: 0.0476, &#39;learning_rate&#39;: 8.65515114127082e-06, &#39;epoch&#39;: 0.22}
{&#39;loss&#39;: 0.0446, &#39;learning_rate&#39;: 8.346699568167798e-06, &#39;epoch&#39;: 0.25}
{&#39;loss&#39;: 0.0327, &#39;learning_rate&#39;: 8.038247995064774e-06, &#39;epoch&#39;: 0.28}
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
This SpanMarker model won&#39;t be able to predict 0.172563% of all annotated entities in the evaluation dataset. This is caused by the SpanMarkerModel maximum entity length of 6 words.
These are the frequencies of the missed entities due to maximum entity length out of 3477 total entities:
- 5 missed entities with 7 words (0.143802%)
- 1 missed entities with 10 words (0.028760%)
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;eval_loss&#39;: 0.02650175243616104, &#39;eval_overall_precision&#39;: 0.8974691758598313, &#39;eval_overall_recall&#39;: 0.7968885047536733, &#39;eval_overall_f1&#39;: 0.8441934991606898, &#39;eval_overall_accuracy&#39;: 0.9632217370208637, &#39;eval_runtime&#39;: 20.1351, &#39;eval_samples_per_second&#39;: 102.656, &#39;eval_steps_per_second&#39;: 25.676, &#39;epoch&#39;: 0.28}
{&#39;loss&#39;: 0.0348, &#39;learning_rate&#39;: 7.729796421961752e-06, &#39;epoch&#39;: 0.31}
{&#39;loss&#39;: 0.0378, &#39;learning_rate&#39;: 7.42134484885873e-06, &#39;epoch&#39;: 0.33}
{&#39;loss&#39;: 0.0275, &#39;learning_rate&#39;: 7.112893275755707e-06, &#39;epoch&#39;: 0.36}
{&#39;loss&#39;: 0.0242, &#39;learning_rate&#39;: 6.804441702652684e-06, &#39;epoch&#39;: 0.39}
{&#39;loss&#39;: 0.0255, &#39;learning_rate&#39;: 6.495990129549661e-06, &#39;epoch&#39;: 0.42}
{&#39;loss&#39;: 0.0235, &#39;learning_rate&#39;: 6.187538556446638e-06, &#39;epoch&#39;: 0.44}
{&#39;loss&#39;: 0.0223, &#39;learning_rate&#39;: 5.879086983343616e-06, &#39;epoch&#39;: 0.47}
{&#39;loss&#39;: 0.0183, &#39;learning_rate&#39;: 5.570635410240592e-06, &#39;epoch&#39;: 0.5}
{&#39;loss&#39;: 0.0194, &#39;learning_rate&#39;: 5.26218383713757e-06, &#39;epoch&#39;: 0.53}
{&#39;loss&#39;: 0.0191, &#39;learning_rate&#39;: 4.953732264034547e-06, &#39;epoch&#39;: 0.55}
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
This SpanMarker model won&#39;t be able to predict 0.172563% of all annotated entities in the evaluation dataset. This is caused by the SpanMarkerModel maximum entity length of 6 words.
These are the frequencies of the missed entities due to maximum entity length out of 3477 total entities:
- 5 missed entities with 7 words (0.143802%)
- 1 missed entities with 10 words (0.028760%)
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;eval_loss&#39;: 0.016905048862099648, &#39;eval_overall_precision&#39;: 0.9247838616714698, &#39;eval_overall_recall&#39;: 0.9245174301354077, &#39;eval_overall_f1&#39;: 0.9246506267108485, &#39;eval_overall_accuracy&#39;: 0.9844412097687207, &#39;eval_runtime&#39;: 20.2213, &#39;eval_samples_per_second&#39;: 102.219, &#39;eval_steps_per_second&#39;: 25.567, &#39;epoch&#39;: 0.55}
{&#39;loss&#39;: 0.0206, &#39;learning_rate&#39;: 4.645280690931524e-06, &#39;epoch&#39;: 0.58}
{&#39;loss&#39;: 0.0198, &#39;learning_rate&#39;: 4.336829117828501e-06, &#39;epoch&#39;: 0.61}
{&#39;loss&#39;: 0.0184, &#39;learning_rate&#39;: 4.028377544725479e-06, &#39;epoch&#39;: 0.64}
{&#39;loss&#39;: 0.0203, &#39;learning_rate&#39;: 3.7199259716224557e-06, &#39;epoch&#39;: 0.67}
{&#39;loss&#39;: 0.0206, &#39;learning_rate&#39;: 3.4114743985194327e-06, &#39;epoch&#39;: 0.69}
{&#39;loss&#39;: 0.0187, &#39;learning_rate&#39;: 3.1030228254164097e-06, &#39;epoch&#39;: 0.72}
{&#39;loss&#39;: 0.015, &#39;learning_rate&#39;: 2.794571252313387e-06, &#39;epoch&#39;: 0.75}
{&#39;loss&#39;: 0.0221, &#39;learning_rate&#39;: 2.486119679210364e-06, &#39;epoch&#39;: 0.78}
{&#39;loss&#39;: 0.0189, &#39;learning_rate&#39;: 2.177668106107341e-06, &#39;epoch&#39;: 0.8}
{&#39;loss&#39;: 0.0158, &#39;learning_rate&#39;: 1.8692165330043186e-06, &#39;epoch&#39;: 0.83}
{&#39;eval_loss&#39;: 0.01296199019998312, &#39;eval_overall_precision&#39;: 0.9394202898550724, &#39;eval_overall_recall&#39;: 0.933736675309709, &#39;eval_overall_f1&#39;: 0.9365698598468429, &#39;eval_overall_accuracy&#39;: 0.9868348698043021, &#39;eval_runtime&#39;: 20.2701, &#39;eval_samples_per_second&#39;: 101.973, &#39;eval_steps_per_second&#39;: 25.506, &#39;epoch&#39;: 0.83}
{&#39;loss&#39;: 0.0165, &#39;learning_rate&#39;: 1.5607649599012956e-06, &#39;epoch&#39;: 0.86}
{&#39;loss&#39;: 0.017, &#39;learning_rate&#39;: 1.2523133867982728e-06, &#39;epoch&#39;: 0.89}
{&#39;loss&#39;: 0.0183, &#39;learning_rate&#39;: 9.438618136952499e-07, &#39;epoch&#39;: 0.92}
{&#39;loss&#39;: 0.0164, &#39;learning_rate&#39;: 6.35410240592227e-07, &#39;epoch&#39;: 0.94}
{&#39;loss&#39;: 0.0162, &#39;learning_rate&#39;: 3.2695866748920424e-07, &#39;epoch&#39;: 0.97}
{&#39;loss&#39;: 0.021, &#39;learning_rate&#39;: 1.850709438618137e-08, &#39;epoch&#39;: 1.0}
{&#39;train_runtime&#39;: 479.9392, &#39;train_samples_per_second&#39;: 30.033, &#39;train_steps_per_second&#39;: 3.755, &#39;train_loss&#39;: 0.06940532092560087, &#39;epoch&#39;: 1.0}
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
TrainOutput(global_step=1802, training_loss=0.06940532092560087, metrics={&#39;train_runtime&#39;: 479.9392, &#39;train_samples_per_second&#39;: 30.033, &#39;train_steps_per_second&#39;: 3.755, &#39;train_loss&#39;: 0.06940532092560087, &#39;epoch&#39;: 1.0})
</pre></div></div>
</div>
<p>And now the final step is to compute the model’s performance.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">metrics</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
<span class="n">metrics</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;eval_loss&#39;: 0.012707239016890526,
 &#39;eval_LOC&#39;: {&#39;precision&#39;: 0.9642857142857143,
  &#39;recall&#39;: 0.9503610108303249,
  &#39;f1&#39;: 0.9572727272727273,
  &#39;number&#39;: 1108},
 &#39;eval_MISC&#39;: {&#39;precision&#39;: 0.8805309734513275,
  &#39;recall&#39;: 0.8378947368421052,
  &#39;f1&#39;: 0.8586839266450916,
  &#39;number&#39;: 475},
 &#39;eval_ORG&#39;: {&#39;precision&#39;: 0.8736842105263158,
  &#39;recall&#39;: 0.9021739130434783,
  &#39;f1&#39;: 0.8877005347593583,
  &#39;number&#39;: 736},
 &#39;eval_PER&#39;: {&#39;precision&#39;: 0.9776247848537005,
  &#39;recall&#39;: 0.9861111111111112,
  &#39;f1&#39;: 0.9818496110630942,
  &#39;number&#39;: 1152},
 &#39;eval_overall_precision&#39;: 0.9379688401615696,
 &#39;eval_overall_recall&#39;: 0.9366176894266782,
 &#39;eval_overall_f1&#39;: 0.9372927778578637,
 &#39;eval_overall_accuracy&#39;: 0.9872553776483908,
 &#39;eval_runtime&#39;: 19.9052,
 &#39;eval_samples_per_second&#39;: 103.842,
 &#39;eval_steps_per_second&#39;: 25.973,
 &#39;epoch&#39;: 1.0}
</pre></div></div>
</div>
<p>Additionally, we should evaluate using the test set.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">],</span> <span class="n">metric_key_prefix</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;test_loss&#39;: 0.029485255479812622,
 &#39;test_LOC&#39;: {&#39;precision&#39;: 0.9335384615384615,
  &#39;recall&#39;: 0.9094724220623501,
  &#39;f1&#39;: 0.9213483146067416,
  &#39;number&#39;: 1668},
 &#39;test_MISC&#39;: {&#39;precision&#39;: 0.7503429355281207,
  &#39;recall&#39;: 0.7792022792022792,
  &#39;f1&#39;: 0.76450034940601,
  &#39;number&#39;: 702},
 &#39;test_ORG&#39;: {&#39;precision&#39;: 0.8538243626062323,
  &#39;recall&#39;: 0.9072847682119205,
  &#39;f1&#39;: 0.87974314068885,
  &#39;number&#39;: 1661},
 &#39;test_PER&#39;: {&#39;precision&#39;: 0.9658808933002482,
  &#39;recall&#39;: 0.9628942486085343,
  &#39;f1&#39;: 0.964385258593992,
  &#39;number&#39;: 1617},
 &#39;test_overall_precision&#39;: 0.8947827604257547,
 &#39;test_overall_recall&#39;: 0.9079320113314447,
 &#39;test_overall_f1&#39;: 0.9013094296511117,
 &#39;test_overall_accuracy&#39;: 0.9782276300204588,
 &#39;test_runtime&#39;: 33.9555,
 &#39;test_samples_per_second&#39;: 104.401,
 &#39;test_steps_per_second&#39;: 26.122,
 &#39;epoch&#39;: 1.0}
</pre></div></div>
</div>
<p>Great performance for 8 minutes trained! 🎉</p>
<p>Once trained, we can save our new model locally.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="s2">&quot;models/span-marker-roberta-base-conll03/checkpoint-final&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Or we can push it to the 🤗 Hub like so.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;span-marker-roberta-base-conll03&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>If we want to use it again, we can just load it using the checkpoint or using the model name on the Hub. This is how it would be done using a local checkpoint. See the <a class="reference internal" href="model_loading.html"><span class="doc">Loading &amp; Inferencing</span></a> notebook for more details.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># model = SpanMarkerModel.from_pretrained(&quot;models/span-marker-roberta-base-conll03/checkpoint-final&quot;)</span>
</pre></div>
</div>
</div>
<p>That was all! As simple as that. If we put it all together into a single script, it looks something like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">span_marker</span><span class="w"> </span><span class="kn">import</span> <span class="n">SpanMarkerModel</span><span class="p">,</span> <span class="n">Trainer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">TrainingArguments</span>

<span class="k">def</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">dataset_id</span> <span class="o">=</span> <span class="s2">&quot;conll2003&quot;</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="n">dataset_id</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="s2">&quot;ner_tags&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">feature</span><span class="o">.</span><span class="n">names</span>

    <span class="n">encoder_id</span> <span class="o">=</span> <span class="s2">&quot;roberta-base&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">SpanMarkerModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="c1"># Required arguments</span>
        <span class="n">encoder_id</span><span class="p">,</span>
        <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
        <span class="c1"># Optional arguments</span>
        <span class="n">model_max_length</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
        <span class="n">entity_max_length</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
        <span class="c1"># To improve the generated model card</span>
        <span class="n">model_card_data</span><span class="o">=</span><span class="n">SpanMarkerModelCardData</span><span class="p">(</span>
            <span class="n">language</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;en&quot;</span><span class="p">],</span>
            <span class="n">license</span><span class="o">=</span><span class="s2">&quot;apache-2.0&quot;</span><span class="p">,</span>
            <span class="n">encoder_id</span><span class="o">=</span><span class="n">encoder_id</span><span class="p">,</span>
            <span class="n">dataset_id</span><span class="o">=</span><span class="n">dataset_id</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
        <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;models/span-marker-roberta-base-conll03&quot;</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s2">&quot;steps&quot;</span><span class="p">,</span>
        <span class="n">save_strategy</span><span class="o">=</span><span class="s2">&quot;steps&quot;</span><span class="p">,</span>
        <span class="n">eval_steps</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
        <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">logging_steps</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">warmup_ratio</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
        <span class="n">train_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">8000</span><span class="p">)),</span>
        <span class="n">eval_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;validation&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">)),</span>
    <span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="n">metrics</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="s2">&quot;models/span-marker-roberta-base-conll03/checkpoint-final&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p>With <code class="docutils literal notranslate"><span class="pre">wandb</span></code> initialized, you can enjoy their very useful training graphs straight in your browser. It ends up looking something like this. <img alt="image" src="https://user-images.githubusercontent.com/37621491/235172501-a3cdae91-faf0-42b7-ac60-e6738b78e67e.png" /> <img alt="image1" src="https://user-images.githubusercontent.com/37621491/235172726-795ded55-4b1c-40fa-ab91-476762f7dd57.png" /></p>
<p>Furthermore, you can use the <code class="docutils literal notranslate"><span class="pre">wandb</span></code> hyperparameter search functionality using the tutorial from the Hugging Face documentation <a class="reference external" href="https://huggingface.co/docs/transformers/hpo_train">here</a>. This transfers very well to the SpanMarker <code class="docutils literal notranslate"><span class="pre">Trainer</span></code>.</p>
</section>
</section>


        </div>
      </div>

    </div>

<footer>
    <div id="footer-info">
        <ul id="build-details">
            
                <li class="footer-element">
                    
                        <a href="../_sources/notebooks/model_training.ipynb.txt" rel="nofollow"> source</a>
                    
                </li>
            

            

            
        </ul>

        
            <div id="copyright">
                &copy; 2025, Tom Aarsen
            </div>
        

        <div id="credit">
            created with <a href="http://sphinx-doc.org/">Sphinx</a> and <a href="https://github.com/tomaarsen/nltk_theme">NLTK Theme</a>
        </div>
    </div>
</footer> 

</div>

</body>
</html>