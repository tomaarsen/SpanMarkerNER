<!DOCTYPE html>
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#2D2D2D" />
  
  <title>SpanMarker :: span_marker.data_collator</title>
  
  <link rel="index" title="Index" href="../../genindex.html"/>

  <link rel="stylesheet" href="../../_static/css/nltk_theme.css"/>
  <link rel="stylesheet" href="../../_static/css/custom.css"/>

  <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
  

  <script src="https://email.tl.fortawesome.com/c/eJxNjUEOgyAQAF8jR7Kw6wIHDh7sP1Cw2mgxgmn6-3JsMqc5zEQfE8dkxOY1KKMUOI3ACFKRJpSW2AAp7ontYIaxI6i7XPJVwyeVfCQ550Os3jLrGSNOLgbdAy6s0PBk2TFNjEbsfq31LB0OnX407pJa5v2faRadwSW63mn5KuLyR9j2tgx3zecanl-55R_-jjPs"></script> 
</head>

<body>
  <div id="nltk-theme-container">
    <header>
      <div id="logo-container">
          
          <h1>
            <a href="../../index.html">SpanMarker</a>
          </h1>
          
      </div>
      <div id="project-container">
        
        <h1>Documentation</h1>
        
      </div>

      <a id="menu-toggle" class="fa fa-bars" aria-hidden="true"></a>

      <script type="text/javascript">
        $("#menu-toggle").click(function() {
          $("#menu-toggle").toggleClass("toggled");
          $("#side-menu-container").slideToggle(300);
        });
      </script>
    </header>

    <div id="content-container">

      <div id="side-menu-container">

        <div id="search" role="search">
        <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
            <input type="text" name="q" placeholder="Search" />
            <input type="hidden" name="check_keywords" value="yes" />
            <input type="hidden" name="area" value="default" />
        </form>
</div>

        <div id="side-menu" role="navigation">
          
  
    
  
  
    <p class="caption" role="heading"><span class="caption-text">Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/index.html">Notebooks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/getting_started.html">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/model_training.html">Initializing &amp; Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/model_loading.html">Loading &amp; Inferencing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/model_configuration.html">Configurating</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../api/span_marker.html">API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../api/span_marker.modeling.html">span_marker.modeling module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../api/span_marker.trainer.html">span_marker.trainer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../api/span_marker.configuration.html">span_marker.configuration module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../api/span_marker.data_collator.html">span_marker.data_collator module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../api/span_marker.tokenizer.html">span_marker.tokenizer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../api/span_marker.evaluation.html">span_marker.evaluation module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../api/span_marker.label_normalizer.html">span_marker.label_normalizer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../api/span_marker.output.html">span_marker.output module</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installing SpanMarker</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../news.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/tomaarsen/SpanMarkerNER/issues">Open Issues</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/tomaarsen/SpanMarkerNER">SpanMarker on GitHub</a></li>
</ul>

  

        </div>

        
      </div>

      <div id="main-content-container">
        <div id="main-content" role="main">
          
  <h1>Source code for span_marker.data_collator</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="kn">from</span> <span class="nn">span_marker.tokenizer</span> <span class="kn">import</span> <span class="n">SpanMarkerTokenizer</span>


<div class="viewcode-block" id="SpanMarkerDataCollator"><a class="viewcode-back" href="../../api/span_marker.data_collator.html#span_marker.data_collator.SpanMarkerDataCollator">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">SpanMarkerDataCollator</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Data Collator class responsible for converting the minimal outputs from the tokenizer into</span>
<span class="sd">    complete and meaningful inputs to the model. In particular, the ``input_ids`` from the tokenizer</span>
<span class="sd">    features are padded, and the correct amount of start and end markers (with padding) are added.</span>

<span class="sd">    Furthermore, the position IDs are generated for the input IDs, and ``start_position_ids`` and</span>
<span class="sd">    ``end_position_ids`` are used alongside some padding to create a full position ID vector.</span>

<span class="sd">    Lastly, the attention matrix is computed.</span>

<span class="sd">    The expected usage is something like:</span>

<span class="sd">    &gt;&gt;&gt; collator = SpanMarkerDataCollator(...)</span>
<span class="sd">    &gt;&gt;&gt; tokenized = tokenizer(...)</span>
<span class="sd">    &gt;&gt;&gt; batch = collator(tokenized)</span>
<span class="sd">    &gt;&gt;&gt; output = model(**batch)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">tokenizer</span><span class="p">:</span> <span class="n">SpanMarkerTokenizer</span>
    <span class="n">marker_max_length</span><span class="p">:</span> <span class="nb">int</span>

<div class="viewcode-block" id="SpanMarkerDataCollator.__call__"><a class="viewcode-back" href="../../api/span_marker.data_collator.html#span_marker.data_collator.SpanMarkerDataCollator.__call__">[docs]</a>    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert the minimal tokenizer outputs into inputs ready for :meth:`~span_marker.modeling.SpanMarkerModel.forward`.</span>

<span class="sd">        Args:</span>
<span class="sd">            features (List[Dict[str, Any]]): A list of dictionaries, one element per sample in the batch.</span>
<span class="sd">                The dictionaries contain the following keys:</span>

<span class="sd">                * ``input_ids``: The non-padded input IDs.</span>
<span class="sd">                * ``num_spans``: The number of spans that should be encoded in each sample.</span>
<span class="sd">                * ``start_position_ids``: The position IDs of the start markers in the sample.</span>
<span class="sd">                * ``end_position_ids``: The position IDs of the end markers in the sample.</span>
<span class="sd">                * ``labels`` (optional): The labels corresponding to each of the spans in the sample.</span>
<span class="sd">                * ``num_words`` (optional): The number of words in the input sample.</span>
<span class="sd">                    Required for some evaluation metrics.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, torch.Tensor]: Batch dictionary ready to be fed into :meth:`~span_marker.modeling.SpanMarkerModel.forward`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">total_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">model_max_length</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">marker_max_length</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
        <span class="n">num_words</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">start_marker_indices</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">num_marker_pairs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>
            <span class="n">num_spans</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[</span><span class="s2">&quot;num_spans&quot;</span><span class="p">]</span>
            <span class="n">num_tokens</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

            <span class="c1"># The start markers start after the input IDs, rounded up to the nearest even number</span>
            <span class="n">start_marker_idx</span> <span class="o">=</span> <span class="n">num_tokens</span> <span class="o">+</span> <span class="n">num_tokens</span> <span class="o">%</span> <span class="mi">2</span>
            <span class="n">end_marker_idx</span> <span class="o">=</span> <span class="n">start_marker_idx</span> <span class="o">+</span> <span class="n">num_spans</span>

            <span class="c1"># Prepare input_ids by padding and adding start and end markers</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">total_size</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)),</span> <span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>
            <span class="n">input_ids</span><span class="p">[</span><span class="n">start_marker_idx</span> <span class="p">:</span> <span class="n">start_marker_idx</span> <span class="o">+</span> <span class="n">num_spans</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">start_marker_id</span>
            <span class="n">input_ids</span><span class="p">[</span><span class="n">end_marker_idx</span> <span class="p">:</span> <span class="n">end_marker_idx</span> <span class="o">+</span> <span class="n">num_spans</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">end_marker_id</span>
            <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

            <span class="c1"># Prepare position IDs</span>
            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">position_ids</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">total_size</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">position_ids</span><span class="p">)),</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">position_ids</span><span class="p">[</span><span class="n">start_marker_idx</span> <span class="p">:</span> <span class="n">start_marker_idx</span> <span class="o">+</span> <span class="n">num_spans</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s2">&quot;start_position_ids&quot;</span><span class="p">])</span>
            <span class="n">position_ids</span><span class="p">[</span><span class="n">end_marker_idx</span> <span class="p">:</span> <span class="n">end_marker_idx</span> <span class="o">+</span> <span class="n">num_spans</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s2">&quot;end_position_ids&quot;</span><span class="p">])</span>
            <span class="c1"># Increase the position_ids by 2, inspired by PL-Marker. The intuition is that these position IDs</span>
            <span class="c1"># better match the circumstances under which the underlying encoders are trained.</span>
            <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">position_ids</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>

            <span class="c1"># Prepare attention mask matrix</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">total_size</span><span class="p">,</span> <span class="n">total_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
            <span class="c1"># text tokens self-attention</span>
            <span class="n">attention_mask</span><span class="p">[:</span><span class="n">num_tokens</span><span class="p">,</span> <span class="p">:</span><span class="n">num_tokens</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="c1"># let markers attend text tokens</span>
            <span class="n">attention_mask</span><span class="p">[</span><span class="n">start_marker_idx</span> <span class="p">:</span> <span class="n">start_marker_idx</span> <span class="o">+</span> <span class="n">num_spans</span><span class="p">,</span> <span class="p">:</span><span class="n">num_tokens</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">attention_mask</span><span class="p">[</span><span class="n">end_marker_idx</span> <span class="p">:</span> <span class="n">end_marker_idx</span> <span class="o">+</span> <span class="n">num_spans</span><span class="p">,</span> <span class="p">:</span><span class="n">num_tokens</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="c1"># self-attentions of start/end markers</span>
            <span class="n">start_index_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">start_marker_idx</span><span class="p">,</span> <span class="n">start_marker_idx</span> <span class="o">+</span> <span class="n">num_spans</span><span class="p">))</span>
            <span class="n">end_index_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">end_marker_idx</span><span class="p">,</span> <span class="n">end_marker_idx</span> <span class="o">+</span> <span class="n">num_spans</span><span class="p">))</span>
            <span class="n">attention_mask</span><span class="p">[</span><span class="n">start_index_list</span><span class="p">,</span> <span class="n">start_index_list</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">attention_mask</span><span class="p">[</span><span class="n">start_index_list</span><span class="p">,</span> <span class="n">end_index_list</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">attention_mask</span><span class="p">[</span><span class="n">end_index_list</span><span class="p">,</span> <span class="n">start_index_list</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">attention_mask</span><span class="p">[</span><span class="n">end_index_list</span><span class="p">,</span> <span class="n">end_index_list</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>

            <span class="c1"># Add start of the markers, so the model knows where the input IDs end and where the markers start</span>
            <span class="n">start_marker_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">start_marker_idx</span><span class="p">)</span>
            <span class="n">num_marker_pairs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">end_marker_idx</span> <span class="o">-</span> <span class="n">start_marker_idx</span><span class="p">)</span>

            <span class="k">if</span> <span class="s2">&quot;labels&quot;</span> <span class="ow">in</span> <span class="n">sample</span><span class="p">:</span>
                <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">])</span>
                <span class="n">labels</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">total_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)),</span> <span class="n">value</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>
                <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

            <span class="k">if</span> <span class="s2">&quot;num_words&quot;</span> <span class="ow">in</span> <span class="n">sample</span><span class="p">:</span>
                <span class="n">num_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s2">&quot;num_words&quot;</span><span class="p">])</span>

        <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="c1"># Used for evaluation, does not need to be padded/stacked</span>
        <span class="k">if</span> <span class="n">num_words</span><span class="p">:</span>
            <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;num_words&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">num_words</span><span class="p">)</span>
        <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;start_marker_indices&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">start_marker_indices</span><span class="p">)</span>
        <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;num_marker_pairs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">num_marker_pairs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">batch</span></div></div>
</pre></div>

        </div>
      </div>

    </div>

<footer>
    <div id="footer-info">
        <ul id="build-details">
            

            

            
        </ul>

        
            <div id="copyright">
                &copy; 2023, Tom Aarsen
            </div>
        

        <div id="credit">
            created with <a href="http://sphinx-doc.org/">Sphinx</a> and <a href="https://github.com/tomaarsen/nltk_theme">NLTK Theme</a>
        </div>
    </div>
</footer> 

</div>

</body>
</html>